# Monte Carlo Benchmark Engine (AVX2 / NEON / Threaded)

[![CI](https://github.com/yushasama/montecarlo-benchmarking-engine/actions/workflows/ci.yml/badge.svg)](https://github.com/yushasama/montecarlo-benchmarking-engine/actions)

A high-performance SIMD Monte Carlo engine to estimate PI. written in C++17, benchmarked via `perf` and validated via CI.

> ðŸ”¬ Originally extended from a Spring 2025 CSULB CECS 325 (Systems Programming) assignment by Neal Terrell.

---

## ðŸ§© Features

* **Execution Models**:

  * Sequential
  * Heap-allocated
  * Custom bump allocator (thread-local, reset-based)
  * SIMD-accelerated (AVX2 / NEON)

* **Memory Optimization**:

  * Preallocated, thread-local memory pools for allocation-free reuse
  * Structure-of-Arrays layout for SIMD-friendly access patterns
  * Optimized to reduce false sharing through thread-local state and separation of write paths
  * Improved L1/L2 cache locality via contiguous memory and low-fragmentation allocation

* **Performance Profiling (optional)**:

  * `perf stat` integration: IPC, cache/TLB misses, branch mispredictions
  * Tracks cycles-per-trial and miss-per-trial metrics

* **Logging & Analysis**:

  * Zstandard-compressed Parquet output
  * Auto-generated Markdown performance tables for quick inspection

* **CI Tested**: GitHub Actions verifies builds and logs per commit

* **Cross-platform**:

  * âœ… Linux and macOS supported
  * âš ï¸ Windows support (MSVC + Ninja / MinGW) is experimental

---

## ðŸ§  Requirements

* CMake 3.15+
* Ninja
* Clang++ (recommended) or GCC 12+
* Python 3.10+ with `pip`

### Optional (for Performance Logging & Grafana Dashboard)

If you want to benchmark with `perf` and view results:

* `perf` (Linux only)
* `clickhousedb` (for Grafana ingest)
* `polars` (for CSV parsing)
* `grafana` for dashboard UI

---

## âš™ï¸ Setup Instructions

### ðŸ§ Arch Linux
```bash
sudo pacman -S cmake ninja perf sqlite duckdb python python-pip
yay -S grafana frser-sqlite-datasource
pip install polars duckdb

---

## ðŸ”§ Build (Linux/macOS/WSL)

```bash
cmake -G Ninja -B build
ninja -C build
./build/montecarlo 100000000 SIMD
```

You can also test other methods:

```bash
./build/montecarlo 100000000 Sequential
./build/montecarlo 100000000 Heap
./build/montecarlo 100000000 Pool
./build/montecarlo 100000000 SIMD
```

### ðŸªŸ Windows Support

#### âœ… WSL (Windows Subsystem for Linux)

Fully supported. Use the same Linux instructions above. Allows `perf`, Python, and native benchmarking.

#### âš ï¸ MSVC (Developer Prompt)

Partially supported. Build via Ninja:

```cmd
cmake -G Ninja -B build -DCMAKE_CXX_FLAGS="/O2"
ninja -C build
build\montecarlo.exe 100000000 SIMD
```

Some allocators or SIMD intrinsics may require patching.

#### âŒ MinGW

Not officially supported. May fail on memory alignment or `std::allocator_traits` features.

---

## ðŸ“Š Run Full Benchmark Script (Optional)

> This script uses Linux `perf` and logs results as Parquet + Markdown.

```bash
chmod +x scripts/run_perf.sh
./scripts/run_perf.sh 100000000
```

By default, runs all methods (Sequential, Heap, Pool, SIMD).

---

## ðŸ¤– GitHub Actions CI

Every push or PR to `main` is automatically tested via GitHub Actions:

* Builds using Clang + CMake + Ninja on Ubuntu
* Runs a dry smoke test for all methods:

  ```bash
  ./build/montecarlo 10000 Sequential
  ./build/montecarlo 10000 Heap
  ./build/montecarlo 10000 Pool
  ./build/montecarlo 10000 SIMD
  ```
* CI ensures correctness, not benchmarking

CI badge and logs: [View GitHub Actions](https://github.com/yushasama/montecarlo-benchmarking-engine/actions)

---

## ðŸ Performance Snapshot (Optional)

> Generated via `scripts/run_perf.sh`, parsed via `parse_perf_metrics.py`, and logged via `gen_perf_parquet_logs.py`

ðŸ“¦ **Parquet snapshot example:**

```bash
logs/batch_c6d4dcc6_2025-05-13_17-47-41/perf_results_SIMD_2025-05-13_17-47-41_c6d4dcc6.parquet
```

To view `.parquet` files visually, visit: [https://www.tablab.app](https://www.tablab.app) and drag the file in.

Nice â€” if youâ€™ve got a `samples/` folder with example `.parquet` and `.md` outputs for people to inspect, you can phrase it like this:

---

## ðŸ“„ Markdown Table Summary

A sample Markdown snapshot is included for reference:
ðŸ‘‰ [samples/perf_results.md](./samples/perf_results_all_c6d4dcc6_sample.md)

This contains a clean summary of benchmark results across all methods and is auto-generated by the pipeline.

To inspect raw `.parquet` logs directly, explore files in:

```bash
samples/*.parquet
```

You can also view them visually via [https://www.tablab.app](https://www.tablab.app) â€” just drag and drop any `.parquet` file.

---