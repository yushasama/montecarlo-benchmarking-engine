<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Monte Carlo Benchmarking Engine: About</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Monte Carlo Benchmarking Engine
   </div>
   <div id="projectbrief">High-performance SIMD Monte Carlo engine (AVX2/NEON) with custom memory allocators and perf logging.</div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('index.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">About </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a></p>
<p><a href="https://github.com/yushasama/montecarlo-benchmarking-engine/actions"><img src="https://github.com/yushasama/montecarlo-benchmarking-engine/actions/workflows/ci.yml/badge.svg" alt="CI" style="pointer-events: none;" class="inline"/></a></p>
<p>A high-performance SIMD Monte Carlo engine to estimate PI. written in C++17, benchmarked via <code>perf</code> and validated via CI. Engine is built to be highly optimized through SIMD aware programming (AVX2 / Neon), multi-threading, custom bump allocator, and cache-aligned memory strategies, bitmasking.</p>
<p>Benchmarked using an in-house <code>perf</code> suite, and tested via CI.</p>
<blockquote class="doxtable">
<p>üî¨ Originally based on a Spring 2025 CSULB CECS 325 (Systems Programming) assignment by Neal Terrell. Highly extended and tuned by Leon - just for fun. </p>
</blockquote>
<blockquote class="doxtable">
<p>œÄ is estimated by randomly sampling (x, y) points in a unit square and counting how many fall inside the unit circle ‚Äî the ratio approximates œÄ/4. Which is then multiplied by 4 to get our approximation of œÄ. </p>
</blockquote>
<hr  />
<p>‚Üí <a href="https://yushasama.github.io/montecarlo-benchmarking-engine/">View Online Documentation</a></p>
<hr  />
<h1><a class="anchor" id="autotoc_md-table-of-contents"></a>
üìö Table of Contents</h1>
<ol type="1">
<li><a class="el" href="#autotoc_md-features">Features</a></li>
<li><a class="el" href="#autotoc_md-core-optimization-techniques">Core Optimization Techniques</a><ul>
<li><a class="el" href="#autotoc_md1-distance-check-formula">Distance Check Formula</a></li>
<li><a class="el" href="#autotoc_md2-simd-accelerated-trial-execution">SIMD</a></li>
<li><a class="el" href="#autotoc_md3-bitmasking-fast-hit-counting-with-no-branches">Bitmasking</a></li>
<li><a class="el" href="#autotoc_md4-memory-optimization-pool-allocator">Memory Pool Allocator</a></li>
<li><a class="el" href="#autotoc_md5-thread-local-everything-no-locks">Thread-Local Design</a></li>
</ul>
</li>
<li><a class="el" href="#real-world-analogy-high-frequency-trading-hft">Real-World Analogy: HFT</a></li>
<li><a class="el" href="#benchmark-oriented-design-summary">Benchmark-Oriented Design Summary</a></li>
<li>Tips for Reviewers / Recruiters</li>
<li><a class="el" href="#requirements">Requirements</a></li>
<li><a class="el" href="#setup-instructions">Setup Instructions</a><ul>
<li><a class="el" href="#autotoc_md-arch-linux">Arch Linux</a></li>
<li><a class="el" href="#autotoc_md-linux">Linux</a></li>
<li><a class="el" href="#autotoc_md-macos-with-homebrew">macOS</a></li>
<li><a class="el" href="#autotoc_md-windows-wsl2---recommended">Windows (WSL2)</a></li>
<li>Windows (MSVC)</li>
<li><a class="el" href="#autotoc_md-windows-mingw---not-supported">Windows (MinGW)</a></li>
</ul>
</li>
<li><a class="el" href="#building--running">Building &amp; Running</a></li>
<li><a class="el" href="#autotoc_md-running-benchmark-suite-optional">Benchmark Suite (Optional)</a></li>
<li><a class="el" href="#autotoc_md-docker-optional-for-clickhouse--grafana-setup--data-visualization">Docker + Grafana Integration</a></li>
<li><a class="el" href="#Ô∏è-perf-dashboard-setup-docker--makefile">Perf Dashboard Setup (Docker + Makefile)</a></li>
<li><a class="el" href="#autotoc_md-environment-configuration-env">Environment Configuration</a></li>
<li><a class="el" href="#autotoc_md-github-actions-ci">GitHub Actions CI</a></li>
<li><a class="el" href="#autotoc_md-performance-benchmark-snapshot">Sample Benchmark Logs</a></li>
</ol>
<hr  />
<h1><a class="anchor" id="autotoc_md-features"></a>
üß© Features</h1>
<ul>
<li><b>Execution Models</b>:<ul>
<li>Sequential</li>
<li>Heap-allocated w/ multi threading</li>
<li>Custom bump memory pool allocator (thread-local, reset-based) w/ multi threading</li>
<li>SIMD-accelerated (AVX2 / NEON) w/ memory pool &amp; multi threading</li>
</ul>
</li>
<li><b>Memory Optimization</b>:<ul>
<li>Preallocated, thread-local memory pools for allocation-free reuse</li>
<li>Structure-of-Arrays layout for SIMD-friendly access patterns</li>
<li>Optimized to reduce false sharing through thread-local state and separation of write paths</li>
<li>Improved L1 cache locality via contiguous memory and low-fragmentation allocation</li>
</ul>
</li>
<li><b>Performance Profiling (optional)</b>:<ul>
<li><code>perf stat</code> integration: IPC, cache/TLB misses, branch mispredictions</li>
<li>Tracks cycles-per-trial and miss-per-trial metrics</li>
</ul>
</li>
<li><b>Logging &amp; Analysis</b>:<ul>
<li>Zstandard-compressed Parquet output</li>
<li>Auto-generated CSV performance tables for quick inspection</li>
</ul>
</li>
<li><b>CI Tested</b>: GitHub Actions verifies builds and logs per commit</li>
<li><b>Cross-platform</b>:<ul>
<li>‚úÖ Linux, Windows (WSL), and macOS supported</li>
<li>‚ö†Ô∏è Windows support (MSVC + Ninja) is experimental</li>
</ul>
</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md-core-optimization-techniques"></a>
üîß Core Optimization Techniques</h1>
<p>This section breaks down the internal optimizations that power the engine, with special focus on SIMD, memory, and cache-sensitive design.</p>
<h2><a class="anchor" id="autotoc_md1-distance-check-formula"></a>
1. <b>Distance Check Formula</b></h2>
<p>Traditional distance check uses a square root:</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (sqrt(x*x + y*y) &lt;= 1.0)</div>
</div><!-- fragment --><p>This is replaced by:</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (x*x + y*y) &lt;= 1.0</div>
</div><!-- fragment --><p><b>Why?</b></p>
<ul>
<li>Removes expensive <code>sqrt()</code> call</li>
<li>Enables vectorization (SIMD) and compiler auto-vectorization</li>
</ul>
<hr  />
<h2><a class="anchor" id="autotoc_md2-simd-accelerated-trial-execution"></a>
2. <b>SIMD-Accelerated Trial Execution</b></h2>
<h3><a class="anchor" id="avx2-x86-64"></a>
AVX2 (x86-64)</h3>
<ul>
<li>Uses 256-bit registers (<code>__m256d</code>)</li>
<li>4 double-precision floats per SIMD batch</li>
<li><p class="startli">Circle check is vectorized:</p>
<div class="fragment"><div class="line">_mm256_add_pd(_mm256_mul_pd(x, x), _mm256_mul_pd(y, y)) &lt;= 1.0</div>
</div><!-- fragment --></li>
<li>Result: hit counts are derived via bitmask (<code>_mm256_movemask_pd</code>) and <code>__builtin_popcount</code></li>
</ul>
<h3><a class="anchor" id="neon-arm64"></a>
NEON (ARM64)</h3>
<ul>
<li>128-bit registers (<code>float64x2_t</code>)</li>
<li>2 double-precision floats per batch</li>
<li>Performs <code>vcleq_f64</code> and <code>vmulq_f64</code> in parallel</li>
</ul>
<p>Both methods avoid conditional branches, pipeline stalls, and scalar overhead.</p>
<hr  />
<h3><a class="anchor" id="autotoc_md3-bitmasking-fast-hit-counting-with-no-branches"></a>
3. <b>Bitmasking: Fast Hit Counting with No Branches</b></h3>
<p>After computing whether each dart is inside the circle, SIMD comparisons produce a <b>bitmask</b>:</p>
<div class="fragment"><div class="line"><span class="comment">// For AVX2</span></div>
<div class="line">__m256d cmp = _mm256_cmp_pd(...);</div>
<div class="line"><span class="keywordtype">int</span> mask = _mm256_movemask_pd(cmp);  <span class="comment">// e.g., 0b1010 = 2 hits</span></div>
<div class="line"><span class="keywordtype">int</span> hits = __builtin_popcount(mask); <span class="comment">// Fast hardware popcount</span></div>
</div><!-- fragment --><p><b>Why bitmasking?</b></p>
<ul>
<li>Replaces 4 conditional <code>if</code>s with a single integer mask</li>
<li>Enables <b>branchless counting</b> in constant time</li>
<li>Avoids CPU misprediction penalties from random dart throws</li>
<li>Maps directly to native CPU instructions (fast + deterministic)</li>
</ul>
<blockquote class="doxtable">
<p>Bitmasking is a classic SIMD pattern ‚Äî perfect for Monte Carlo trials where each outcome is independent and binary. </p>
</blockquote>
<hr  />
<h2><a class="anchor" id="autotoc_md4-memory-optimization-pool-allocator"></a>
4. <b>Memory Optimization: Pool Allocator</b></h2>
<h3><a class="anchor" id="what-it-does"></a>
What it does:</h3>
<ul>
<li>Allocates from a fixed-size buffer using a <b>bump pointer</b></li>
<li>No <code>malloc</code>, <code>free</code>, or heap metadata</li>
<li>Memory is reused every frame via <code>reset()</code></li>
</ul>
<h3><a class="anchor" id="why-it-matters"></a>
Why it matters:</h3>
<ul>
<li>Heap allocation causes:<ul>
<li>Fragmentation</li>
<li>Lock contention in multithreaded workloads</li>
<li>Unpredictable latency due to OS-level bookkeeping</li>
</ul>
</li>
<li><a class="el" href="structPoolAllocator.html" title="Fast aligned bump allocator for multithreaded simulations.">PoolAllocator</a> offers:<ul>
<li><b>O(1)</b> allocation</li>
<li><b>Zero fragmentation</b></li>
<li><b>Cache-aligned</b> access (64-byte default)</li>
</ul>
</li>
</ul>
<p>‚ö†Ô∏è Note: For small types like int, the performance difference between heap and pool allocation is minimal, as the allocation cost is quickly amortized.</p>
<p>However, as object size increases‚Äîespecially with larger structs, arrays, or cache-heavy data‚Äîthe heap introduces significant overhead due to metadata, fragmentation, and poor locality.</p>
<p>In contrast, <a class="el" href="structPoolAllocator.html" title="Fast aligned bump allocator for multithreaded simulations.">PoolAllocator</a> maintains constant-time, contiguous allocation, making it dramatically faster and more cache-friendly for large or frequently reused types.</p>
<h3><a class="anchor" id="performance-impact"></a>
Performance Impact:</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Metric   </th><th class="markdownTableHeadNone"><code>new</code> / <code>malloc</code>   </th><th class="markdownTableHeadNone"><code><a class="el" href="structPoolAllocator.html" title="Fast aligned bump allocator for multithreaded simulations.">PoolAllocator</a></code>    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Allocation Latency   </td><td class="markdownTableBodyNone">Variable   </td><td class="markdownTableBodyNone">Constant (1 pointer bump)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Cache Locality   </td><td class="markdownTableBodyNone">Unpredictable   </td><td class="markdownTableBodyNone">Strong (contiguous memory)    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">SIMD Alignment   </td><td class="markdownTableBodyNone">Manual / fragile   </td><td class="markdownTableBodyNone">Default (64B, AVX/NEON safe)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">GC/Freeing   </td><td class="markdownTableBodyNone">Per-object   </td><td class="markdownTableBodyNone">Bulk reset (O(1))   </td></tr>
</table>
<hr  />
<h2><a class="anchor" id="autotoc_md5-thread-local-everything-no-locks"></a>
5. <b>Thread-Local Everything (No Locks)</b></h2>
<p>Each simulation thread gets:</p>
<ul>
<li>Its own RNG (<code>std::mt19937_64</code>)</li>
<li>Its own <code><a class="el" href="structPoolAllocator.html" title="Fast aligned bump allocator for multithreaded simulations.">PoolAllocator</a></code></li>
<li>No shared writes, no atomics</li>
</ul>
<p>Benefits:</p>
<ul>
<li><b>No false sharing</b>: Allocated data is 64B-aligned (cache line size)</li>
<li><b>No heap contention</b>: Each thread manages its own memory space</li>
<li><b>No synchronization</b>: All computation and memory state is isolated</li>
</ul>
<blockquote class="doxtable">
<p>Threads join only at final result aggregation ‚Äî no locks or mutexes are required at any step. </p>
</blockquote>
<hr  />
<h2><a class="anchor" id="autotoc_md6-scalar-fallback-for-remainders"></a>
6. <b>Scalar Fallback for Remainders</b></h2>
<p>SIMD only works on batches (4 trials for AVX2, 2 for NEON).</p>
<p>To maintain precision:</p>
<ul>
<li>Remainder trials (<code>n % batch</code>) fall back to scalar logic</li>
<li>Ensures all <code>n</code> trials are run without vector masking complexity</li>
</ul>
<hr  />
<h1><a class="anchor" id="real-world-analogy-high-frequency-trading-hft"></a>
Real-World Analogy: High-Frequency Trading (HFT)</h1>
<p>This memory model mirrors arena allocators used in HFT systems:</p>
<ul>
<li>Allocate short-lived order/trade structs in a thread-local memory arena</li>
<li>Reset per-tick or per-frame</li>
<li>Avoid GC stalls or malloc overhead during latency-sensitive events</li>
</ul>
<p><b>Why it works here:</b> Monte Carlo trials are <b>embarrassingly parallel</b> and short-lived ‚Äî ideal for memory reuse and SIMD-style vectorization.</p>
<hr  />
<h1><a class="anchor" id="benchmark-oriented-design-summary"></a>
Benchmark-Oriented Design Summary</h1>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Feature   </th><th class="markdownTableHeadNone">Reason    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code><a class="el" href="structPoolAllocator.html" title="Fast aligned bump allocator for multithreaded simulations.">PoolAllocator</a></code>   </td><td class="markdownTableBodyNone">Minimal latency per alloc, avoids fragmentation    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">SIMD (AVX2/NEON)   </td><td class="markdownTableBodyNone">Process 2‚Äì4 trials per instruction cycle    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Thread-Local PRNG   </td><td class="markdownTableBodyNone">Avoids locking / shared access    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">No heap in hot path   </td><td class="markdownTableBodyNone">Eliminates OS allocator variability    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Aligned memory (64B)   </td><td class="markdownTableBodyNone">Safe for AVX/NEON, avoids false sharing    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>reset()</code> reuse model   </td><td class="markdownTableBodyNone">Fast GC-like memory clearing in <code>O(1)</code>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Scalar fallback   </td><td class="markdownTableBodyNone">Completes remaining trials without SIMD masking   </td></tr>
</table>
<hr  />
<h2><a class="anchor" id="autotoc_md-tip-for-reviewers--recruiters--real-world-examples"></a>
üí° Tip for Reviewers / Recruiters + Real World Examples</h2>
<p>The memory, parallelism, and vectorization strategies in this engine directly reflect patterns used in:</p>
<p><b>High Frequency Trading</b> HRT (Hudson River Trading), a high frequency trading firm, uses a pre-allocated pool to allow better usage huge pages to shave off nano seconds in trading operations. This is crucial as in HFT, every tiny unit of time can determine whether or not a profit or loss is made.</p>
<p>&gt;HRT carefully manages memory allocation patterns and alignment, recognizing that poor memory management can lead to cache thrashing and significantly impact performance in latency-critical trading operations.</p>
<p>This project mirrors this by using a bump-style memory pool allocator for linear memory growth, avoiding frequent dynamic allocations, as well as aligning memory to cache lines to reduce L1/L2 thrashing.</p>
<p><a href="https://www.hudsonrivertrading.com/hrtbeat/low-latency-optimization-part-1/">üîó Low Latency Optimizations by HRT Part 1</a> <a href="https://www.hudsonrivertrading.com/hrtbeat/low-latency-optimization-part-12">üîó Low Latency Optimizations by HRT Part 2</a></p>
<p><b>Order book matching engines</b> LMAX, a London-based forex exchange, uses pre-allocation memory for ring buffers instead of tradtional queues for event handling.</p>
<p>&gt;As a result of these practices, overhead from GC (garbage collector) pressure and "contention in multi-threaded implementations." has been minimized.</p>
<p>This project applies the same principle through pre-allocated memory, minimizing thread contention through thread-local memory pool allocators.</p>
<p><a href="https://lmax-exchange.github.io/disruptor/disruptor.html#_memory_allocation">üîó LMAX Disruptor's Whitepaper</a></p>
<p><b>Machine Learning Compilers (eg., Pytorch JIT)</b> In the section, Best Practices for Backends, from the PyTorch developer docs, it is stated that</p>
<p>&gt;"Compiled workloads are usually optimized by Single Instruction Multiple Data (SIMD) instruction sets."</p>
<p>Similarly, this project employs SIMD insruction sets, specifically AVX2 instrinsics (eg: <code>_m256_*</code>), to batch compute dart hits, resulting in massive performance boost.</p>
<p><a href="https://docs.pytorch.org/docs/stable/torch.compiler_best_practices_for_backends">üîó Pytoch's Best Practices for Compiler Design</a></p>
<p><b>Similarity Search (Meta's Faiss Library)</b> Faiss AKA Facebook AI Similarity Search is a hihgly optimized library that allows developers to search multi media documents in ways that tradtional database engines (SQL) do not support as strongly.</p>
<p>This project follows Meta's low latency optimization strategies and for the very same purpose, computing distances.</p>
<p>&gt;Namely, Meta uses multi-threading, SIMD vectorization, and popcount (bitmasking) to speed up distance computations.</p>
<p>Exactly the same methods this project employs and for calculating distance, though Meta's distance computations are more complex!</p>
<p><a href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/">üîó Meta's Blog on Faiss Library</a></p>
<p><b>Video Processing (Netflix's VMAF)</b> Netflix uses VMAF as a video quality metric, originally designed purely for Netflix's streaming use case before its open source. Netflix's engineers has taken great efforts to successfully improve VMAF's performance:</p>
<p>&gt;"Through low-level code optimization and vectorization, we sped up VMAF‚Äôs execution by more than 4x in the past."</p>
<p>Specifically, these speed improvements were carried out by AVX2 &amp; AVX-512 intrisics. Note that AVX2 &amp; AVX-512 are respectively 256-bit and 512-bit registers.</p>
<p>Through out this project, <code>_mm256_*</code> intrinsics are heavily used during the Monte Carlo simulation process and come from the AVX2 instruction set.</p>
<p><a href="https://netflixtechblog.com/toward-a-better-quality-metric-for-the-video-community-7ed94e752a30">üîó Netflix's VMAF Optimizations Blog</a></p>
<hr  />
<h1><a class="anchor" id="requirements"></a>
Requirements</h1>
<ul>
<li>CMake 3.15+</li>
<li>Ninja</li>
<li>Clang++ (recommended) or GCC 12+</li>
</ul>
<hr  />
<h1><a class="anchor" id="setup-instructions"></a>
Setup Instructions</h1>
<p>Note: <code>perf</code> is not supported on macOS or WSL. Use a bare metal Linux setup if you want benchmarking</p>
<h2><a class="anchor" id="autotoc_md-arch-linux"></a>
üêß Arch Linux</h2>
<div class="fragment"><div class="line">sudo pacman -S cmake ninja clang</div>
</div><!-- fragment --><hr  />
<h2><a class="anchor" id="autotoc_md-linux"></a>
üåÄ Linux</h2>
<div class="fragment"><div class="line">sudo apt update</div>
<div class="line">sudo apt install cmake ninja-build clang</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md-macos-with-homebrew"></a>
üçé macOS (with Homebrew)</h2>
<div class="fragment"><div class="line">brew install cmake ninja</div>
</div><!-- fragment --><hr  />
<h2><a class="anchor" id="autotoc_md-windows-wsl2---recommended"></a>
ü™ü Windows (WSL2 ‚Äî ‚úÖ Recommended)</h2>
<p><b>WSL (Windows Subsystem for Linux)</b> is fully supported. Setup is identical to Linux:</p>
<div class="fragment"><div class="line">sudo apt update</div>
<div class="line">sudo apt install cmake ninja-build clang python3-pip</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>‚úÖ Clang and AVX2 work on WSL with native Linux tooling. ‚ùå Direct Windows builds are not supported due to lack of <code>std::aligned_alloc</code> and allocator trait compatibility. </p>
</blockquote>
<h2><a class="anchor" id="autotoc_md-windows-msvc--Ô∏è-experimental"></a>
ü™ü Windows (MSVC ‚Äî ‚ö†Ô∏è Experimental)</h2>
<p>Partial support via Ninja inside Developer Prompt:</p>
<div class="fragment"><div class="line">cmake -G Ninja -B build</div>
<div class="line">ninja -C build</div>
</div><!-- fragment --><p>Some allocators or SIMD intrinsics may require patching.</p>
<h2><a class="anchor" id="autotoc_md-windows-mingw---not-supported"></a>
ü™ü Windows (MinGW ‚Äî ‚ùå Not Supported)</h2>
<p>MinGW is <b>not supported</b> due to lack of <code>std::aligned_alloc</code> and <code>std::allocator_traits</code> compatibility.</p>
<hr  />
<h1><a class="anchor" id="building--running"></a>
Building &amp; Running</h1>
<h3><a class="anchor" id="build-linuxmacoswsl"></a>
Build (Linux/macOS/WSL)</h3>
<div class="fragment"><div class="line">cmake -G Ninja -B build</div>
<div class="line">ninja -C build</div>
<div class="line">./build/montecarlo [TRIALS] [METHOD]</div>
</div><!-- fragment --><p>Running just <code>./build/montecarlo</code> runs all methods with <code>1,000,000,000</code> trials each by default.</p>
<p>Note that <code>[TRIALS]</code> and <code>[METHOD]</code> are optional parameters and default to running <code>1,000,000,000</code> trials and all execution methods respectively.</p>
<p>You can also singly test other execution models:</p>
<div class="fragment"><div class="line">./build/montecarlo 100000000 Sequential</div>
<div class="line">./build/montecarlo 100000000 Heap</div>
<div class="line">./build/montecarlo 100000000 Pool</div>
<div class="line">./build/montecarlo 100000000 SIMD</div>
</div><!-- fragment --><hr  />
<h2><a class="anchor" id="autotoc_md-running-benchmark-suite-optional"></a>
üìä Running Benchmark Suite (Optional)</h2>
<p>This uses Linux perf and <em>requires bare metal Linux.</em></p>
<p>Results are logged in .parquet (primarily for effiency) and partial support for .csv (for readability).</p>
<div class="fragment"><div class="line">chmod +x scripts/run_perf.sh</div>
<div class="line">./scripts/run_perf.sh              # runs all methods with 1,000,000,000 trials</div>
<div class="line">./scripts/run_perf.sh [TRIALS] [METHODS]</div>
<div class="line">./scripts/run_perf.sh 50000000 SIMD insert_db=false  # Skip ClickHouse inser</div>
</div><!-- fragment --><p>By default:</p>
<ul>
<li>All methods are benchmarked</li>
<li>Results are exported as <code>.parquet</code> files</li>
<li>Results are inserted into ClickHouse automatically each run.</li>
</ul>
<blockquote class="doxtable">
<p>Thus you must have Clickhouse already running with the specified configs from your .env file. You can do this by running <code>make init</code> or <code>make init_demo</code> if its your first time. Otherwise run <code>make up</code> (docker-compose up -d). </p>
</blockquote>
<blockquote class="doxtable">
<p>Pass <code>insert_db=false</code> to skip inserting (e.g., for CI or dry runs). </p>
</blockquote>
<p>Note that <code>/scripts/run_perf.sh [TRIALS] [METHODS]</code> is to be treated the same as running <code>./build/montecarlo [TRIALS] [METHODS]</code></p>
<h2><a class="anchor" id="autotoc_md-docker-optional-for-clickhouse--grafana-setup--data-visualization"></a>
üêã Docker (Optional for ClickHouse + Grafana Setup / Data Visualization)</h2>
<blockquote class="doxtable">
<p>Used for dashboard visualization and log ingestion. </p>
</blockquote>
<p>If you want to use the full monitoring pipeline:</p>
<ol type="1">
<li>Install Docker + Docker Compose üëâ Follow instructions for your OS:<ul>
<li><a href="https://docs.docker.com/engine/install/">Docker for Linux</a></li>
<li><a href="https://www.docker.com/products/docker-desktop/">Docker Desktop for macOS/Windows</a></li>
</ul>
</li>
</ol>
<p><b>Benchmark Suite Snapshot</b> <img src="https://files.catbox.moe/loir4s.png" alt="" class="inline"/></p>
<hr  />
<h2><a class="anchor" id="Ô∏è-perf-dashboard-setup-docker--makefile"></a>
üõ†Ô∏è Perf Dashboard Setup (Docker + Makefile)</h2>
<p>This project includes a <code>Makefile</code> to manage your local Docker environment for setting up Clickhouse and loading data, log ingestion + visualization via ClickHouse and Grafana.</p>
<blockquote class="doxtable">
<p>üßº These commands manage everything from booting the stack to loading demo data and resetting volumes. </p>
</blockquote>
<h3><a class="anchor" id="autotoc_md-makefile-command-table"></a>
üìã Makefile Command Table</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Command   </th><th class="markdownTableHeadNone">Description    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make start</code>   </td><td class="markdownTableBodyNone">üê≥ Start Docker containers (ClickHouse, Grafana)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make stop</code>   </td><td class="markdownTableBodyNone">üì¶ Stop containers, <b>preserve data</b>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make rebuild</code>   </td><td class="markdownTableBodyNone">üîÑ Restart + rebuild images (data preserved)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make reset_all</code>   </td><td class="markdownTableBodyNone">üßº Full reset (‚ö†Ô∏è <b>deletes volumes</b>) and rebuilds    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make clean_all</code>   </td><td class="markdownTableBodyNone">üßπ Remove Docker volumes + local data (dangerous!)    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make clear_data</code>   </td><td class="markdownTableBodyNone">üìÅ Deletes local simulation data only (<code>db/</code>)    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make clear_parquets</code>   </td><td class="markdownTableBodyNone">üßΩ Deletes all local <code>.parquet</code> logs    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make logs</code>   </td><td class="markdownTableBodyNone">üìú Streams Docker logs from all containers    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make init</code>   </td><td class="markdownTableBodyNone">üå± Start stack and initialize ClickHouse schema    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make init_demo</code>   </td><td class="markdownTableBodyNone">üå∏ Load sample data (<code>db_sample.parquet</code>) into ClickHouse    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make load_data</code>   </td><td class="markdownTableBodyNone">üì• Load your current simulation log (<code>db/db.parquet</code>) into ClickHouse    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>make load_demo</code>   </td><td class="markdownTableBodyNone">üß∫ Load demo Parquet into <code>db/</code>, then into ClickHouse    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>make setup_clickhouse</code>   </td><td class="markdownTableBodyNone">üõ†Ô∏è Manually reinitialize ClickHouse schema   </td></tr>
</table>
<blockquote class="doxtable">
<p>‚ö†Ô∏è <b>Important:</b> Any of the following commands will <b>overwrite all current ClickHouse data</b> by reloading from <code>DB_PATH</code>: <code>make init_demo</code>, <code>make load_data</code>, <code>make load_demo</code>, or any command that invokes <code><a class="el" href="namespacescripts_1_1setup.html">scripts.setup</a> --load-*</code>. </p>
</blockquote>
<hr  />
<h2><a class="anchor" id="autotoc_md-environment-configuration-env"></a>
üìÑ Environment Configuration (<code>.env</code>)</h2>
<p>You must configure your environment before running the Docker stack.</p>
<p>A <code>.env.sample</code> file is included. To get started:</p>
<div class="fragment"><div class="line">cp .env.sample .env</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>You may tweak the values, but they should work out of the box <b>if ports <code>9000</code>, <code>8123</code>, and <code>3000</code> are free</b>. </p>
</blockquote>
<h3><a class="anchor" id="default-env-variables"></a>
Default <code>.env</code> Variables</h3>
<div class="fragment"><div class="line"># ClickHouse</div>
<div class="line">CLICKHOUSE_USER=default</div>
<div class="line">CLICKHOUSE_PASSWORD=</div>
<div class="line">CLICKHOUSE_UID=clickhouse-datasource</div>
<div class="line"> </div>
<div class="line"># For Python client</div>
<div class="line">CLICKHOUSE_HOST=localhost</div>
<div class="line">CLICKHOUSE_TCP_PORT=9000</div>
<div class="line">CLICKHOUSE_HTTP_PORT=8123</div>
<div class="line"> </div>
<div class="line"># For Docker containers</div>
<div class="line">CLICKHOUSE_HOST_DOCKER=clickhouse</div>
<div class="line">CLICKHOUSE_TCP_PORT_DOCKER=9000</div>
<div class="line">CLICKHOUSE_HTTP_PORT_DOCKER=8123</div>
<div class="line"> </div>
<div class="line"># Grafana</div>
<div class="line">GRAFANA_PORT=3000</div>
<div class="line"> </div>
<div class="line"># Data paths</div>
<div class="line">DB_PATH=db/db.parquet</div>
<div class="line">SAMPLE_PATH=samples/db_sample.parquet</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>If you experience port conflicts (e.g., port 3000 is in use), either:</p>
<ul>
<li>Kill the conflicting service</li>
<li>Or <b>edit <code>.env</code></b> to use alternate ports (and reflect those changes in your <code>docker-compose.yml</code>) </li>
</ul>
</blockquote>
<hr  />
<h2><a class="anchor" id="autotoc_md-github-actions-ci"></a>
ü§ñ GitHub Actions CI</h2>
<p>Every push or PR to <code>main</code> is automatically tested via GitHub Actions:</p>
<ul>
<li>Builds using Clang + CMake + Ninja on Ubuntu</li>
<li><p class="startli">Runs a dry smoke test for all methods:</p>
<div class="fragment"><div class="line">./build/montecarlo 10000 Sequential</div>
<div class="line">./build/montecarlo 10000 Heap</div>
<div class="line">./build/montecarlo 10000 Pool</div>
<div class="line">./build/montecarlo 10000 SIMD</div>
</div><!-- fragment --></li>
<li>CI ensures correctness, not benchmarking</li>
</ul>
<p>CI badge and logs: <a href="https://github.com/yushasama/montecarlo-benchmarking-engine/actions">View GitHub Actions</a></p>
<hr  />
<h2><a class="anchor" id="autotoc_md-performance-benchmark-snapshot"></a>
üìÑ Performance Benchmark Snapshot</h2>
<p>A sample snapshot is included for reference: üëâ samples/perf_results_all_4ca37783.md</p>
<p>Note that markdowk files are not generated automatically, the markdown file was generated for ease of display for this readme.</p>
<p>To inspect raw <code>.parquet</code> logs directly, explore files in:</p>
<div class="fragment"><div class="line">samples/*.parquet</div>
</div><!-- fragment --><p>You can also view them visually via <a href="https://www.tablab.app">https://www.tablab.app</a> ‚Äî just drag and drop any <code>.parquet</code> file.</p>
<hr  />
 </div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
